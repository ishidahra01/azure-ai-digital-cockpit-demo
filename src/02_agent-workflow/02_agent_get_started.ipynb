{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Cockpit demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Environmental Recognition Agent**\n",
    "---\n",
    "- **Role:** This agent is responsible for understanding the environment.\n",
    "- **Functions:** \n",
    "  - Recognizes user behavior and the surrounding context.\n",
    "  - Creates a \"Context\" to inform subsequent agents about the current environment.\n",
    "- **Components:**\n",
    "  - Speech recognition module.\n",
    "  - Environmental recognition module.\n",
    "  - Environmental sensor processing module.\n",
    "- **Demo Architecture:**\n",
    "  - Prepare the sample dataset\n",
    "  - The agent consists of a singble SLM(Phi-3.5 vision). Use multimodal version to support image input.\n",
    "  - Uge Phi-3.5 vision model with Serverless API that is provided by [Azure AI Model Catalog](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-catalog?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_agent_client = ChatCompletionsClient(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT_PHI35_VISION\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_INFERENCE_CREDENTIAL_PHI35_VISION\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: phi35-vision-instruct\n",
      "Model type: chat-completion\n",
      "Model provider name: Phi\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the model\n",
    "env_agent_model_info = env_agent_client.get_model_info()\n",
    "print(\"Model name:\", env_agent_model_info.model_name)\n",
    "print(\"Model type:\", env_agent_model_info.model_type)\n",
    "print(\"Model provider name:\", env_agent_model_info.model_provider_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to be sent to the model\n",
    "\n",
    "# Vehicle data\n",
    "VehicleData = {\n",
    "    \"InteriorTemperature\": 25.5,  # Interior temperature (°C)\n",
    "    \"ExteriorTemperature\": 30.2,  # Exterior temperature (°C)\n",
    "    \"Humidity\": 60,  # Humidity (%)\n",
    "    \"Weather\": \"Sunny\",  # Weather condition\n",
    "    \"Speed\": \"80km/h\",  # Vehicle speed\n",
    "    \"Acceleration\": \"1.2m/s²\",  # Acceleration\n",
    "    \"SteeringAngle\": \"15°\",  # Steering wheel angle\n",
    "    \"FuelLevel\": 45,  # Fuel level (%)\n",
    "    \"EngineRPM\": 2500,  # Engine revolutions per minute (RPM)\n",
    "    \"BatteryVoltage\": 12.8  # Battery voltage (V)\n",
    "}\n",
    "\n",
    "# Driver state\n",
    "DriverState = {\n",
    "    \"HeartRate\": 72,  # Driver's heart rate (BPM)\n",
    "    \"EyeStatus\": \"Open\",  # Eye status\n",
    "    \"Yawning\": False,  # Yawning detection\n",
    "    \"AttentionLevel\": \"High\"  # Attention level\n",
    "}\n",
    "\n",
    "# Window status\n",
    "WindowStatus = {\n",
    "    \"FrontLeft\": \"Closed\",  # Front left window\n",
    "    \"FrontRight\": \"Closed\",  # Front right window\n",
    "    \"RearLeft\": \"Open\",  # Rear left window\n",
    "    \"RearRight\": \"Closed\"  # Rear right window\n",
    "}\n",
    "\n",
    "# Door status\n",
    "DoorStatus = {\n",
    "    \"FrontLeft\": \"Closed\",  # Front left door\n",
    "    \"FrontRight\": \"Closed\",  # Front right door\n",
    "    \"RearLeft\": \"Closed\",  # Rear left door\n",
    "    \"RearRight\": \"Closed\",  # Rear right door\n",
    "    \"Trunk\": \"Closed\"  # Trunk\n",
    "}\n",
    "\n",
    "# CAN data\n",
    "CANData = {\n",
    "    \"BrakePressure\": 2.3,  # Brake pressure (bar)\n",
    "    \"ThrottlePosition\": 25.4,  # Throttle position (%)\n",
    "    \"GearPosition\": \"Drive\",  # Gear position\n",
    "    \"TurnSignal\": \"Off\"  # Turn signal status\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt definition\n",
    "env_agent_system_message = f\"\"\"\n",
    "You are an Environmental Recognition Agent.\n",
    "Based on the data collected from the car's sensors, generate JSON-format data that represents the current state of the vehicle and driver.\n",
    "\"\"\"\n",
    "\n",
    "env_agent_user_message = f\"\"\"\n",
    "# Input\n",
    "\"VehicleData\": {VehicleData},\n",
    "\"DriverState\": {DriverState},\n",
    "\"WindowStatus\": {WindowStatus},\n",
    "\"DoorStatus\": {DoorStatus},\n",
    "\"CANData\": {CANData}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "env_agent_response = env_agent_client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=env_agent_system_message),\n",
    "        UserMessage(content=env_agent_user_message),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  {\n",
      "  \"VehicleData\": {\n",
      "    \"InteriorTemperature\": 25.5,\n",
      "    \"ExteriorTemperature\": 30.2,\n",
      "    \"Humidity\": 60,\n",
      "    \"Weather\": \"Sunny\",\n",
      "    \"Speed\": 80,\n",
      "    \"Acceleration\": 1.2,\n",
      "    \"SteeringAngle\": 15,\n",
      "    \"FuelLevel\": 45,\n",
      "    \"EngineRPM\": 2500,\n",
      "    \"BatteryVoltage\": 12.8\n",
      "  },\n",
      "  \"DriverState\": {\n",
      "    \"HeartRate\": 72,\n",
      "    \"EyeStatus\": \"Open\",\n",
      "    \"Yawning\": false,\n",
      "    \"AttentionLevel\": \"High\"\n",
      "  },\n",
      "  \"WindowStatus\": {\n",
      "    \"FrontLeft\": \"Closed\",\n",
      "    \"FrontRight\": \"Closed\",\n",
      "    \"RearLeft\": \"Open\",\n",
      "    \"RearRight\": \"Closed\"\n",
      "  },\n",
      "  \"DoorStatus\": {\n",
      "    \"FrontLeft\": \"Closed\",\n",
      "    \"FrontRight\": \"Closed\",\n",
      "    \"RearLeft\": \"Closed\",\n",
      "    \"RearRight\": \"Closed\",\n",
      "    \"Trunk\": \"Closed\"\n",
      "  },\n",
      "  \"CANData\": {\n",
      "    \"BrakePressure\": 2.3,\n",
      "    \"ThrottlePosition\": 25.4,\n",
      "    \"GearPosition\": \"Drive\",\n",
      "    \"TurnSignal\": \"Off\"\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Model: phi35-vision-instruct\n",
      "Usage:\n",
      "\tPrompt tokens: 350\n",
      "\tTotal tokens: 716\n",
      "\tCompletion tokens: 366\n"
     ]
    }
   ],
   "source": [
    "print(\"Response:\", env_agent_response.choices[0].message.content)\n",
    "print(\"Model:\", env_agent_response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\\tPrompt tokens:\", env_agent_response.usage.prompt_tokens)\n",
    "print(\"\\tTotal tokens:\", env_agent_response.usage.total_tokens)\n",
    "print(\"\\tCompletion tokens:\", env_agent_response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proposal Agent**\n",
    "---\n",
    "- **Role:** Proposes actions or suggestions based on the recognized environment and user intent.\n",
    "- **Functions:**\n",
    "  - Performs intent analysis to understand user needs.\n",
    "  - Sets target values for actions or goals.\n",
    "  - Calculates discrepancies and defines resolution strategies.\n",
    "- **Features:** Includes a user preference processing function to make personalized suggestions.\n",
    "- **Demo Architecture:**\n",
    "  - Use the output from Environmental Recognition Agent as input for this Proposal Agent\n",
    "  - The agent consists of a singble SLM(Phi-3.5).\n",
    "  - Uge Phi-3.5 model with Serverless API that is provided by [Azure AI Model Catalog](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-catalog?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_agent_client = ChatCompletionsClient(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT_PHI35\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_INFERENCE_CREDENTIAL_PHI35\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: phi35-mini-instruct\n",
      "Model type: chat-completion\n",
      "Model provider name: Phi\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the model\n",
    "proposal_agent_model_info = proposal_agent_client.get_model_info()\n",
    "print(\"Model name:\", proposal_agent_model_info.model_name)\n",
    "print(\"Model type:\", proposal_agent_model_info.model_type)\n",
    "print(\"Model provider name:\", proposal_agent_model_info.model_provider_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_action_agents = [\n",
    "\t\"temperature_control_agent\",\n",
    "\t\"window_control_agent\",\n",
    "\t\"entertainment_control_agent\",\n",
    "]\n",
    "\n",
    "user_preferences = {\n",
    "\t\"preferred_temperature\": 20,\n",
    "\t\"preferred_humidity\": 40,\n",
    "\t\"preferred_noise_level\": \"low\",\n",
    "\t\"preferred_air_quality\": \"good\",\n",
    "\t\"preferred_lighting\": \"dim\",\n",
    "\t\"preferred_music\": \"cheerful\",\n",
    "\t\"favorite_places\": [\"beach\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_example = {\n",
    "    \"target_agent\": \"Temperature Control Agent\",\n",
    "    \"ideal_state\": {\n",
    "        \"temperature\": 26,\n",
    "        \"humidity\": 50\n",
    "    },\n",
    "    \"instructions\": {\n",
    "        \"action\": \"set_temperature\",\n",
    "        \"value\": 26\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt definition\n",
    "proposal_agent_system_message = f\"\"\"\n",
    "You are an agent tasked with leveraging in-car systems to provide an optimal and comfortable environment for the user.\n",
    "Based on the current state, user preferences, and available action agents, suggest an ideal comfortable state.\n",
    "Then, determine the appropriate agent and the instructions needed to achieve the ideal state.\n",
    "You MUST output ONLY the json as shown in the output example. You MUST NOT includie any other additional comment or text.\n",
    "\"\"\"\n",
    "\n",
    "proposal_agent_user_message = f\"\"\"\n",
    "# Input\n",
    "\n",
    "## Current Environment State\n",
    "{env_agent_response.choices[0].message.content}\n",
    "\n",
    "## Available Action Agents\n",
    "{available_action_agents}\n",
    "\n",
    "## User Preferences\n",
    "{user_preferences}\n",
    "\n",
    "# Output example:\n",
    "{output_example}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "proposal_agent_response = proposal_agent_client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=proposal_agent_system_message),\n",
    "        UserMessage(content=proposal_agent_user_message),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  {\n",
      "  \"target_agent\": \"temperature_control_agent\",\n",
      "  \"ideal_state\": {\"temperature\": 20, \"humidity\": 40},\n",
      "  \"instructions\": {\"action\": \"set_temperature\", \"value\": 20, \"humidity\": 40}\n",
      "}\n",
      "\n",
      "Model: phi35-mini-instruct\n",
      "Usage:\n",
      "\tPrompt tokens: 657\n",
      "\tTotal tokens: 734\n",
      "\tCompletion tokens: 77\n"
     ]
    }
   ],
   "source": [
    "print(\"Response:\", proposal_agent_response.choices[0].message.content)\n",
    "print(\"Model:\", proposal_agent_response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\\tPrompt tokens:\", proposal_agent_response.usage.prompt_tokens)\n",
    "print(\"\\tTotal tokens:\", proposal_agent_response.usage.total_tokens)\n",
    "print(\"\\tCompletion tokens:\", proposal_agent_response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processing Agent**\n",
    "---\n",
    "- **Role:** Executes tasks or commands based on the proposals.\n",
    "- **Functions:**\n",
    "  - Handles external service skills and edge-local skills.\n",
    "  - Executes predefined \"Skills\" to perform user-requested tasks.\n",
    "- **Notes:** Skills can include controlling devices, retrieving information, or performing other actionable commands.\n",
    "- **Demo Architecture:**\n",
    "  - Use the output from Proposal Agent as input for this Processing Agent\n",
    "  - The agent consists of a singble SLM(Phi-3.5).\n",
    "  - Uge Phi-3.5 model with Serverless API that is provided by [Azure AI Model Catalog](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-catalog?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Control Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def get_azureml_inference(messages):\n",
    "\n",
    "    allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "    # Request data goes here\n",
    "    # The example below assumes JSON formatting which may be updated\n",
    "    # depending on the format your endpoint expects.\n",
    "    # More information can be found here:\n",
    "    # https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "    data = {\n",
    "        \"input_data\": messages,\n",
    "        \"params\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"do_sample\": True,\n",
    "            \"return_full_text\": False\n",
    "        }\n",
    "    }\n",
    "\n",
    "    body = str.encode(json.dumps(data))\n",
    "\n",
    "    url = os.environ[\"AZURE_INFERENCE_ENDPOINT_PHI35_TEMPERATURE\"]\n",
    "    # Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "    api_key = os.environ[\"AZURE_INFERENCE_CREDENTIAL_PHI35_TEMPERATURE\"]\n",
    "    if not api_key:\n",
    "        raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "\n",
    "    headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "\n",
    "        result = response.read()\n",
    "        decoded_result = result.decode('utf-8')\n",
    "        parsed_result = json.loads(decoded_result)\n",
    "        print(parsed_result)\n",
    "        return parsed_result[\"result\"]\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.read().decode(\"utf8\", 'ignore'))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of in-vehicle API. This might be generated based on the actual API specification, document, code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_control_list_available_features = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_cabin_temperature\",\n",
    "            \"description\": \"Set cabin temperature to specified degree.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"temperature\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Target temperature to be set in the cabin.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"temperature\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"auto_adjust_cabin_environment\",\n",
    "            \"description\": \"Automatically adjust the cabin environment.\",\n",
    "            \"strict\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_fan_speed\",\n",
    "            \"description\": \"Set fan speed to specified level.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"fan_speed\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"enum\": [1, 2, 3, 4, 5],\n",
    "                        \"description\": \"Fan speed level (1: Low, 5: High).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"fan_speed\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_seat_heater_level\",\n",
    "            \"description\": \"Set seat heater to specified level.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"level\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"enum\": [0, 1, 2, 3],\n",
    "                        \"description\": \"Seat heater level (0: Off, 3: Max).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"level\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_seat_ventilation_level\",\n",
    "            \"description\": \"Set seat ventilation to specified level.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"level\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"enum\": [0, 1, 2, 3],\n",
    "                        \"description\": \"Seat ventilation level (0: Off, 3: Max).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"level\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_steering_heater_level\",\n",
    "            \"description\": \"Set steering heater to specified level.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"level\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"enum\": [0, 1, 2],\n",
    "                        \"description\": \"Steering heater level (0: Off, 2: Max).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"level\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt definition\n",
    "temperature_control_agent_system_message = f\"\"\"\n",
    "You are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\\n<tools>\\n{temperature_control_list_available_features}\\n</tools>\\nFor each function call return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{{'arguments': <args-dict>, 'name': <function-name>}}\\n</tool_call>\\n\n",
    "\"\"\"\n",
    "\n",
    "temperature_control_agent_user_message = f\"\"\"\n",
    "# Input\n",
    "\n",
    "## Proposal Contents from the Proposal Agent\n",
    "{proposal_agent_response.choices[0].message.content}\n",
    "\n",
    "## User Preferences\n",
    "{user_preferences}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "\t{\"role\": \"system\", \"content\": temperature_control_agent_system_message},\n",
    "\t{\"role\": \"user\", \"content\": temperature_control_agent_user_message},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': ' <tool_call>\\n{\"name\": \"set_cabin_temperature\", \"arguments\": {\"temperature\": 20}}\\n</tool_call>'}\n"
     ]
    }
   ],
   "source": [
    "response = get_azureml_inference(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low grade model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_control_list_available_features_low = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_cabin_temperature\",\n",
    "            \"description\": \"Set cabin temperature to specified degree.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"temperature\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Target temperature to be set in the cabin.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"temperature\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"set_fan_speed\",\n",
    "            \"description\": \"Set fan speed to specified level.\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"fan_speed\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"enum\": [1, 2, 3, 4, 5],\n",
    "                        \"description\": \"Fan speed level (1: Low, 5: High).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"fan_speed\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt definition for low grade model\n",
    "temperature_control_agent_system_message_low = f\"\"\"\n",
    "You are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\\n<tools>\\n{temperature_control_list_available_features_low}\\n</tools>\\nFor each function call return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{{'arguments': <args-dict>, 'name': <function-name>}}\\n</tool_call>\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_low=[\n",
    "\t{\"role\": \"system\", \"content\": temperature_control_agent_system_message_low},\n",
    "\t{\"role\": \"user\", \"content\": temperature_control_agent_user_message},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': \" <tool_call>\\n{'arguments': {'temperature': 20}, 'name':'set_cabin_temperature'}\\n</tool_call>\"}\n"
     ]
    }
   ],
   "source": [
    "response = get_azureml_inference(messages_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users will evaluate the actions performed by the AI agent, judging whether they are satisfactory or not. If they are dissatisfied with an action, they will likely take corrective measures themselves (e.g., raising the temperature if the AI lowered it too much).\n",
    "\n",
    "To consistently reflect user preferences, the system will analyze the history of AI agent actions and corresponding user responses. This data will be used to periodically update the user profile.\n",
    "\n",
    "The updated profile will then serve as prompot for the AI agent to determine and execute actions it deems more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_preferences = {\n",
    "\t\"preferred_temperature\": 28, # Updated preferred temperature!\n",
    "\t\"preferred_humidity\": 40,\n",
    "\t\"preferred_noise_level\": \"low\",\n",
    "\t\"preferred_air_quality\": \"good\",\n",
    "\t\"preferred_lighting\": \"dim\",\n",
    "\t\"preferred_music\": \"cheerful\",\n",
    "\t\"favorite_places\": [\"beach\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_agent_user_message = f\"\"\"\n",
    "# Input\n",
    "\n",
    "## Current Environment State\n",
    "{env_agent_response.choices[0].message.content}\n",
    "\n",
    "## Available Action Agents\n",
    "{available_action_agents}\n",
    "\n",
    "## User Preferences\n",
    "{user_preferences}\n",
    "\n",
    "# Output example:\n",
    "{output_example}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "proposal_agent_response = proposal_agent_client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=proposal_agent_system_message),\n",
    "        UserMessage(content=proposal_agent_user_message),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  {\n",
      "  \"target_agent\": \"temperature_control_agent\",\n",
      "  \"ideal_state\": {\n",
      "    \"temperature\": 28,\n",
      "    \"humidity\": 40\n",
      "  },\n",
      "  \"instructions\": {\n",
      "    \"action\": \"set_temperature\",\n",
      "    \"value\": 28,\n",
      "    \"humidity\": 40\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Response:\", proposal_agent_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_control_agent_user_message = f\"\"\"\n",
    "# Input\n",
    "\n",
    "## Proposal Contents from the Proposal Agent\n",
    "{proposal_agent_response.choices[0].message.content}\n",
    "\n",
    "## User Preferences\n",
    "{user_preferences}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "\t{\"role\": \"system\", \"content\": temperature_control_agent_system_message},\n",
    "\t{\"role\": \"user\", \"content\": temperature_control_agent_user_message},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': ' <tool_call>\\n{\"name\": \"set_cabin_temperature\", \"arguments\": {\"temperature\": 28}}\\n</tool_call>'}\n"
     ]
    }
   ],
   "source": [
    "response = get_azureml_inference(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
